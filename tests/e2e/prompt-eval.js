#!/usr/bin/env node

/**
 * Script d'√©valuation des r√©ponses LLM avec LangSmith
 * √âvalue la qualit√© des r√©ponses g√©n√©r√©es par l'AI App Builder
 */

const fs = require('fs');
const path = require('path');
const { LangChainTracer } = require('langchain/callbacks');
const { ChatOpenAI } = require('langchain/chat_models/openai');
const { PromptTemplate } = require('langchain/prompts');
const { StringOutputParser } = require('langchain/output_parsers');

// Configuration LangSmith
const LANGSMITH_API_KEY = process.env.LANGSMITH_API_KEY;
const LANGSMITH_PROJECT = process.env.LANGSMITH_PROJECT || 'ai-app-builder-evals';
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

if (!LANGSMITH_API_KEY) {
  console.error('‚ùå LANGSMITH_API_KEY est requis');
  process.exit(1);
}

if (!OPENAI_API_KEY) {
  console.error('‚ùå OPENAI_API_KEY est requis');
  process.exit(1);
}

// Configuration du tracer LangSmith
const tracer = new LangChainTracer({
  projectName: LANGSMITH_PROJECT,
  apiKey: LANGSMITH_API_KEY,
});

// Mod√®le d'√©valuation
const evaluatorModel = new ChatOpenAI({
  openAIApiKey: OPENAI_API_KEY,
  modelName: 'gpt-4',
  temperature: 0,
});

/**
 * √âvaluateur de qualit√© du code g√©n√©r√©
 */
class CodeQualityEvaluator {
  constructor() {
    this.evaluationPrompt = new PromptTemplate({
      template: `
Vous √™tes un expert en d√©veloppement web et √©valuation de code. 
√âvaluez la qualit√© du code g√©n√©r√© par l'AI App Builder.

PROMPT ORIGINAL:
{prompt}

CODE G√âN√âR√â:
{generated_code}

CRIT√àRES D'√âVALUATION:
1. Qualit√© du code (structure, lisibilit√©, bonnes pratiques)
2. Compl√©tude (toutes les fonctionnalit√©s demand√©es sont impl√©ment√©es)
3. Utilisation de TypeScript (types, interfaces, g√©n√©riques)
4. Patterns React (hooks, composants, √©tat)
5. S√©curit√© (validation, sanitisation)
6. Performance (optimisations, lazy loading)
7. Accessibilit√© (ARIA, s√©mantique)
8. Tests (pr√©sence de tests unitaires)

R√©pondez au format JSON suivant:
{{
  "overall_score": 0.85,
  "criteria_scores": {{
    "code_quality": 0.9,
    "completeness": 0.8,
    "typescript_usage": 0.85,
    "react_patterns": 0.9,
    "security": 0.7,
    "performance": 0.8,
    "accessibility": 0.75,
    "tests": 0.6
  }},
  "pass": true,
  "feedback": "Code de tr√®s bonne qualit√© avec quelques am√©liorations possibles",
  "improvements": [
    "Ajouter plus de tests unitaires",
    "Am√©liorer la gestion d'erreurs",
    "Optimiser les performances avec React.memo"
  ],
  "strengths": [
    "Excellent usage de TypeScript",
    "Bons patterns React",
    "Code bien structur√©"
  ]
}}
`,
      inputVariables: ['prompt', 'generated_code'],
    });
  }

  async evaluate(prompt, generatedCode) {
    try {
      const chain = this.evaluationPrompt
        .pipe(evaluatorModel)
        .pipe(new StringOutputParser());

      const result = await chain.invoke(
        { prompt, generated_code: generatedCode },
        { callbacks: [tracer] }
      );

      return JSON.parse(result);
    } catch (error) {
      console.error('‚ùå Erreur lors de l\'√©valuation:', error);
      return {
        overall_score: 0,
        pass: false,
        error: error.message
      };
    }
  }
}

/**
 * √âvaluateur de fonctionnalit√©
 */
class FunctionalityEvaluator {
  constructor() {
    this.functionalityPrompt = new PromptTemplate({
      template: `
√âvaluez si le code g√©n√©r√© impl√©mente correctement les fonctionnalit√©s demand√©es.

PROMPT ORIGINAL:
{prompt}

CODE G√âN√âR√â:
{generated_code}

V√©rifiez:
1. Toutes les fonctionnalit√©s demand√©es sont pr√©sentes
2. Le code est fonctionnel et ex√©cutable
3. Les d√©pendances sont correctement import√©es
4. La structure du projet est logique

R√©pondez au format JSON:
{{
  "functionality_score": 0.9,
  "missing_features": [],
  "present_features": ["formulaire", "validation", "API"],
  "executable": true,
  "dependencies_correct": true,
  "structure_logical": true,
  "pass": true
}}
`,
      inputVariables: ['prompt', 'generated_code'],
    });
  }

  async evaluate(prompt, generatedCode) {
    try {
      const chain = this.functionalityPrompt
        .pipe(evaluatorModel)
        .pipe(new StringOutputParser());

      const result = await chain.invoke(
        { prompt, generated_code: generatedCode },
        { callbacks: [tracer] }
      );

      return JSON.parse(result);
    } catch (error) {
      console.error('‚ùå Erreur lors de l\'√©valuation de fonctionnalit√©:', error);
      return {
        functionality_score: 0,
        pass: false,
        error: error.message
      };
    }
  }
}

/**
 * √âvaluateur de s√©curit√©
 */
class SecurityEvaluator {
  constructor() {
    this.securityPrompt = new PromptTemplate({
      template: `
√âvaluez la s√©curit√© du code g√©n√©r√©.

CODE G√âN√âR√â:
{generated_code}

V√©rifiez:
1. Validation des entr√©es utilisateur
2. Protection contre XSS
3. Protection contre CSRF
4. Gestion s√©curis√©e des mots de passe
5. Authentification et autorisation
6. Sanitisation des donn√©es
7. Gestion des erreurs sans exposition d'informations sensibles

R√©pondez au format JSON:
{{
  "security_score": 0.8,
  "security_issues": [],
  "security_strengths": ["validation", "sanitisation"],
  "pass": true,
  "recommendations": []
}}
`,
      inputVariables: ['generated_code'],
    });
  }

  async evaluate(generatedCode) {
    try {
      const chain = this.securityPrompt
        .pipe(evaluatorModel)
        .pipe(new StringOutputParser());

      const result = await chain.invoke(
        { generated_code: generatedCode },
        { callbacks: [tracer] }
      );

      return JSON.parse(result);
    } catch (error) {
      console.error('‚ùå Erreur lors de l\'√©valuation de s√©curit√©:', error);
      return {
        security_score: 0,
        pass: false,
        error: error.message
      };
    }
  }
}

/**
 * Fonction principale d'√©valuation
 */
async function evaluateTestResults() {
  console.log('üîç D√©but de l\'√©valuation des r√©sultats de tests...');

  // Lire les r√©sultats de tests
  const testResultsPath = path.join(__dirname, '../../test-results');
  if (!fs.existsSync(testResultsPath)) {
    console.error('‚ùå Aucun r√©sultat de test trouv√©');
    process.exit(1);
  }

  const evaluators = {
    codeQuality: new CodeQualityEvaluator(),
    functionality: new FunctionalityEvaluator(),
    security: new SecurityEvaluator()
  };

  const evaluationResults = [];

  // Parcourir les fichiers de r√©sultats
  const resultFiles = fs.readdirSync(testResultsPath);
  
  for (const file of resultFiles) {
    if (file.endsWith('.json')) {
      try {
        const testResult = JSON.parse(fs.readFileSync(path.join(testResultsPath, file), 'utf8'));
        
        // Extraire les donn√©es de test
        const testData = extractTestData(testResult);
        
        if (testData) {
          console.log(`üìä √âvaluation du test: ${testData.testName}`);
          
          // √âvaluations
          const codeQualityEval = await evaluators.codeQuality.evaluate(
            testData.prompt,
            testData.generatedCode
          );
          
          const functionalityEval = await evaluators.functionality.evaluate(
            testData.prompt,
            testData.generatedCode
          );
          
          const securityEval = await evaluators.security.evaluate(
            testData.generatedCode
          );

          // Calculer le score global
          const overallScore = (
            codeQualityEval.overall_score * 0.4 +
            functionalityEval.functionality_score * 0.4 +
            securityEval.security_score * 0.2
          );

          const evaluationResult = {
            testName: testData.testName,
            timestamp: new Date().toISOString(),
            overallScore,
            evaluations: {
              codeQuality: codeQualityEval,
              functionality: functionalityEval,
              security: securityEval
            },
            pass: overallScore >= 0.7
          };

          evaluationResults.push(evaluationResult);
          
          console.log(`‚úÖ Test ${testData.testName}: Score ${overallScore.toFixed(2)} (${evaluationResult.pass ? 'PASS' : 'FAIL'})`);
        }
      } catch (error) {
        console.error(`‚ùå Erreur lors du traitement de ${file}:`, error.message);
      }
    }
  }

  // G√©n√©rer le rapport d'√©valuation
  await generateEvaluationReport(evaluationResults);
  
  console.log('‚úÖ √âvaluation termin√©e');
}

/**
 * Extraire les donn√©es de test des r√©sultats
 */
function extractTestData(testResult) {
  try {
    // Chercher les donn√©es de test dans la structure des r√©sultats Playwright
    for (const spec of testResult.specs || []) {
      for (const test of spec.tests || []) {
        for (const result of test.results || []) {
          if (result.status === 'passed') {
            // Extraire les donn√©es du test (√† adapter selon votre structure)
            const testName = test.title;
            const prompt = extractPromptFromTest(test);
            const generatedCode = extractGeneratedCodeFromTest(test);
            
            if (prompt && generatedCode) {
              return {
                testName,
                prompt,
                generatedCode
              };
            }
          }
        }
      }
    }
  } catch (error) {
    console.error('‚ùå Erreur lors de l\'extraction des donn√©es:', error);
  }
  
  return null;
}

/**
 * Extraire le prompt du test (√† adapter selon votre impl√©mentation)
 */
function extractPromptFromTest(_test) {
  // Impl√©mentation √† adapter selon votre structure de test
  // Par exemple, chercher dans les steps ou les annotations
  return "Prompt extrait du test"; // Placeholder
}

/**
 * Extraire le code g√©n√©r√© du test (√† adapter selon votre impl√©mentation)
 */
function extractGeneratedCodeFromTest(_test) {
  // Impl√©mentation √† adapter selon votre structure de test
  // Par exemple, chercher dans les screenshots ou les logs
  return "Code g√©n√©r√© extrait du test"; // Placeholder
}

/**
 * G√©n√©rer le rapport d'√©valuation
 */
async function generateEvaluationReport(results) {
  const report = {
    timestamp: new Date().toISOString(),
    totalTests: results.length,
    passedTests: results.filter(r => r.pass).length,
    failedTests: results.filter(r => !r.pass).length,
    averageScore: results.reduce((sum, r) => sum + r.overallScore, 0) / results.length,
    results: results
  };

  // Sauvegarder le rapport
  const reportPath = path.join(__dirname, '../../evaluation-report.json');
  fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));

  // Afficher le r√©sum√©
  console.log('\nüìä RAPPORT D\'√âVALUATION');
  console.log('========================');
  console.log(`Total des tests: ${report.totalTests}`);
  console.log(`Tests r√©ussis: ${report.passedTests}`);
  console.log(`Tests √©chou√©s: ${report.failedTests}`);
  console.log(`Score moyen: ${report.averageScore.toFixed(2)}`);
  console.log(`Taux de r√©ussite: ${((report.passedTests / report.totalTests) * 100).toFixed(1)}%`);

  // Envoyer les r√©sultats √† LangSmith
  if (LANGSMITH_API_KEY) {
    try {
      await sendResultsToLangSmith(report);
      console.log('‚úÖ R√©sultats envoy√©s √† LangSmith');
    } catch (error) {
      console.error('‚ùå Erreur lors de l\'envoi √† LangSmith:', error);
    }
  }
}

/**
 * Envoyer les r√©sultats √† LangSmith
 */
async function sendResultsToLangSmith(_report) {
  // Impl√©mentation de l'envoi √† LangSmith
  // Utiliser l'API LangSmith pour cr√©er des runs d'√©valuation
  console.log('üì§ Envoi des r√©sultats √† LangSmith...');
  
  // Placeholder - √† impl√©menter avec l'API LangSmith
  // const response = await fetch('https://api.smith.langchain.com/runs', {
  //   method: 'POST',
  //   headers: {
  //     'Authorization': `Bearer ${LANGSMITH_API_KEY}`,
  //     'Content-Type': 'application/json'
  //   },
  //   body: JSON.stringify(report)
  // });
}

// Ex√©cuter l'√©valuation si le script est appel√© directement
if (require.main === module) {
  evaluateTestResults().catch(console.error);
}

module.exports = {
  CodeQualityEvaluator,
  FunctionalityEvaluator,
  SecurityEvaluator,
  evaluateTestResults
};
